Follow https://docs.aws.amazon.com/glue/latest/dg/aws-glue-programming-etl-format-iceberg.html
Architectural understanding of iceberg : https://guide.aws.dev/articles/ARXwPJozi_QfeqRPZFEeSu6w
1. under job parameter --datalake-formats value: iceberg
2. --conf spark.sql.extensions=org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions 
--conf spark.sql.catalog.glue_catalog=org.apache.iceberg.spark.SparkCatalog 
--conf spark.sql.catalog.glue_catalog.warehouse=s3://sankari-test/tables/iceberg/
--conf spark.sql.catalog.glue_catalog.catalog-impl=org.apache.iceberg.aws.glue.GlueCatalog 
--conf spark.sql.catalog.glue_catalog.io-impl=org.apache.iceberg.aws.s3.S3FileIO
3. No iceberg connectors required for either jobs

4. Glue 4.0 spark 3.3
5. iceberg table maintenance (VaCUUM and OPTIMIZE): https://iceberg.apache.org/docs/latest/maintenance/ 

-------------------------------------------------------------
from pyspark.sql import SparkSession
from pyspark.context import SparkContext

# Iceberg parameters
CATALOG = 'iceberg_catalog'
WAREHOUSE = 's3://sankari-test/tables/iceberg/org/'

# Iceberg configuration
spark = SparkSession.builder \
    .config(f"spark.sql.catalog.{CATALOG}", "org.apache.iceberg.spark.SparkCatalog") \
    .config(f"spark.sql.catalog.{CATALOG}.warehouse", WAREHOUSE) \
    .config(f"spark.sql.catalog.{CATALOG}.type", "hadoop") \
    .config(f"spark.sql.catalog.{CATALOG}.io-impl", "org.apache.iceberg.aws .s3.S3FileIO") \
    .config(f"spark.sql.extensions","org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions") \
    .getOrCreate()

sc = spark.sparkContext

resource = f"{CATALOG}.hadoopdb.example_2"

spark.sql(f"""
CREATE TABLE {resource}(index string, Name string, Country string,Description string, Founded string, NumberOfEmployees string)
USING iceberg
TBLPROPERTIES('format-version'='2')
""")

spark.sql(f"""
INSERT INTO {resource} VALUES ('101','siva''Aus','vlog','1988', '3')
""")

spark.sql(f"""
SELECT * FROM {resource}
""").show(truncate=False)
-------------------------------------------------------------


import sys
from awsglue.transforms import *
from awsglue.utils import getResolvedOptions
from pyspark.context import SparkContext
from awsglue.context import GlueContext
from awsglue.job import Job

## @params: [JOB_NAME]
args = getResolvedOptions(sys.argv, ['JOB_NAME'])

sc = SparkContext()
glueContext = GlueContext(sc)
spark = glueContext.spark_session
job = Job(glueContext)
job.init(args['JOB_NAME'], args)

df = glueContext.create_dynamic_frame.from_catalog(database="s3_example", table_name="sample_csv")
dataFrame = df.toDF()
dataFrame.createOrReplaceTempView("myparquet")
print("temp view built")
query = f"""   CREATE TABLE glue_catalog.default.icebergtb USING iceberg AS SELECT * FROM myparquet"""
spark.sql(query)

print("iceberg table created ")
spark.sql(f"""INSERT INTO glue_catalog.default.icebergtb VALUES ('Siva','sankari',
'Benton, John B Jr',
'6649 N Blue Gum St',
'New Orleans',
'Orleans',
'LA',
70116,
'504-621-8927',
'504-845-1427',
'jbutt@gmail.com',
'http://www.bentonjohnbjr.com')
""")

dataFrame.writeTo("glue_catalog.default.icebergtb").append()

df = glueContext.create_data_frame.from_catalog(
       database="default",
       table_name="icebergtb")  
print(df.show (5))

job.commit()
-----------------------------------------------------
