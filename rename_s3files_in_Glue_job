https://guide.aws.dev/articles/ARUOkRay2bTFmxDRbRFlqSGA/to-rename-the-target-file-in-s3-after-writing-the-data-to-s3-using-a-glue-job

    import sys
    from awsglue.transforms import * 
    from awsglue.utils import getResolvedOptions
    from pyspark.context import SparkContext
    from awsglue.context import GlueContext
    from awsglue.job import Job
    from awsglue import DynamicFrame 

    #setting the variable for target s3 bucket 
    BUCKET_NAME = 'bucketname'
    PREFIX = 'folder'
    args = getResolvedOptions(sys.argv, ['JOB_NAME']) 
    sc = SparkContext()
    glueContext = GlueContext(sc)
    spark = glueContext.spark_session
    job = Job(glueContext)
    job.init(args['JOB_NAME'], args) 
    datasource0 = glueContext.create_dynamic_frame_from_options(connection_type = "s3", connection_options = {"paths": ["s3://<s3-path>"]}, format = "parquet") #reduce parallelism 
    dataF = datasource0.toDF().coalesce(1)
    DyF = DynamicFrame.fromDF(dataF, glueContext, "DyF") 
    datasink2 = glueContext.write_dynamic_frame.from_options(frame = DyF, connection_type = "s3", connection_options = {"path": "s3://" + BUCKET_NAME + "/" + PREFIX}, format = "csv", transformation_ctx = "datasink2") 
    #After writing the dynamic frames to S3 
    import boto3
    client = boto3.client('s3')
    BUCKET_NAME = 'bucketname'
    PREFIX = 'folder'
    response = client.list_objects(Bucket=BUCKET_NAME,Prefix=PREFIX)

    for obj in response["Contents"]:
        if 'part' in obj['Key']:
            name=obj['Key']

    copy_source = {'Bucket': BUCKET_NAME, 'Key': name}
    copy_key = PREFIX + '/test-obj.csv'
    client.copy_object(CopySource=copy_source, Bucket=BUCKET_NAME, Key=copy_key)
    client.delete_object(Bucket=BUCKET_NAME, Key=name)
    job.commit()

https://stackoverflow.com/questions/61675390/pyspark-renaming-file-in-hdfs
https://stackoverflow.com/questions/58171365/how-to-rename-my-json-generated-by-pyspark
https://stackoverflow.com/questions/34012775/pyspark-and-hdfs-commands

